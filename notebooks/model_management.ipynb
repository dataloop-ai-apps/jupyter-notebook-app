{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Management\n",
        "\n",
        "The Model Management module in Dataloop provides a comprehensive solution for managing the entire lifecycle of your AI models. With this module, you have the flexibility to either use pre-existing public AI models or connect and utilize your own custom models.\n",
        "\n",
        "You can train different versions of your models using data Items and their Annotations, experiment with various hyper-parameters to find the best configuration, and evaluate your models by inferencing over Datasets. The module also allows you to compare the training and evaluation metrics of your models, review false-positive and false-negative instances, and make informed decisions based on the results.\n",
        "\n",
        "Once you have trained and evaluated your models, you can deploy them as a service within Dataloop applications. This enables you to run your models at scale and make them available to a wider audience. Additionally, the Model-Management module provides the tools to build continuous learning pipelines, where your models can continuously learn and improve over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why use Dataloop's model management feature?\n",
        "\n",
        "Dataloop's model management feature enables machine learning engineers to centralize their AI model research, training, and production processes. The models are executed using Dpks, Datasets, and Artifacts. The code required to run the model is contained in Dpk and pushed to the Dataloop cloud-native SaaS platform. The Datasets hold the images used for training or inference and define which images belong to each subset, such as train/validation/test. Models can be obtained from ready-to-use open source algorithms or created from pre-trained models for further fine-tuning or transfer learning. Additionally, one can upload their own models and evaluate performance by viewing training metrics. All models can be incorporated into the Dataloop platform and integrated into the UI through buttons or slots, or added to Pipelines.\n",
        "\n",
        "The Model Comparison feature in the Dataloop platform enables users to view and compare different versions of models all in one place. This can be done by selecting user-specified metrics to evaluate the models.\n",
        "\n",
        "The Model Management system in the platform can work in two ways: \"offline\" mode and \"online\" mode.\n",
        "\n",
        "* In the **offline mode**, users can train and run models locally using their own data and compare the configurations and metrics of different models on the platform. This mode does not require extensive platform integration and can be used after creating dl.Dpk and dl.Model entities. However, it is limited to visualizing metrics that have been exported from the local model training sessions only. The code and weights of the models are not saved in the Dataloop platform, but the metrics can be viewed at a later time.\n",
        "\n",
        "* On the other hand, the **online mode** allows users to train models that can be deployed anywhere within the platform. For instance, users can create a button interface to use their model for inferencing on new data and view the results on the platform. To use this mode, users must create a ModelAdapter class and implement the required functions to access the Dataloop API. This mode includes all the features available in the \"offline\" mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Management in Dataloop's Python SDK\n",
        "\n",
        "Model Management is also available both in the UI web version of the Dataloop platform and in the Dataloop Python SDK.\n",
        "\n",
        "You can find more about how to use models in our [documentetaion on Model Management](https://developers.dataloop.ai/tutorials/model_management/) or [our Python SDK-focused documentation](https://sdk-docs.dataloop.ai/en/latest/repositories.html#module-dtlpy.repositories.models).\n",
        "\n",
        "Additionally, here are some code snippets that may be useful:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0. [Prerequisite] Install a dataset and a model from an existing dpk on the project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the current project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dtlpy as dl\n",
        "\n",
        "project_id = 'project_id'\n",
        "\n",
        "project = dl.projects.get(project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install the app with the dataset and the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dpk = project.dpks.get(dpk_name=\"ml-compare-example\")\n",
        "app = project.apps.install(dpk=dpk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the dataset after the app is installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = project.datasets.get(dataset_name=\"V2 Plant Seedlings Dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Model Get"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = project.models.get(model_name='pretrained-resnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Model Clone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create a new copy of our existing model, run the following command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "clone_model = model.clone(model_name='my pretrained resnet',\n",
        "                          description='pretrained cloned into my project',\n",
        "                          project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Model Deploy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case that the model is already ready for production, deploy it by the following command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.deploy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Model Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice: Deployed models cannot be trained, so in this step and the next ones, the clone model that was created on [step 2: clone a model](model_management.ipynb#2-clone-a-model) will be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train a model on a dataset we need to:\n",
        "1. Connect a dataset to the model, that will be used for the training.\n",
        "2. Add the train and validation filters to the model, that will help to split the data to train and validation dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_filters = dl.Filters(field=\"metadata.system.tags.train\", values=True)\n",
        "validation_filters = dl.Filters(field=\"metadata.system.tags.validation\", values=True)\n",
        "\n",
        "clone_model.configuration['id_to_label_map'] = None\n",
        "clone_model.configuration['label_to_id_map'] = None\n",
        "clone_model.dataset_id = dataset.id\n",
        "clone_model.ontology_id = dataset.ontologies.list()[0]\n",
        "clone_model.add_subset(subset_name=\"train\", subset_filter=train_filters)\n",
        "clone_model.add_subset(subset_name=\"validation\", subset_filter=validation_filters)\n",
        "clone_model.update(system_metadata=True)\n",
        "\n",
        "execution = clone_model.train(service_config={\n",
        "    \"name\": \"resnet-train-evaluate\",\n",
        "    \"runtime\": {\n",
        "        \"podType\": \"regular-m\",\n",
        "        \"concurrency\": 1,\n",
        "        \"runnerImage\": \"gcr.io/viewo-g/piper/agent/runner/apps/torch-models:0.1.4\",\n",
        "        \"autoscaler\": {\n",
        "            \"type\": \"rabbitmq\",\n",
        "            \"minReplicas\": 0,\n",
        "            \"maxReplicas\": 2,\n",
        "            \"queueLength\": 100\n",
        "        }\n",
        "    }\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wait for the training to finish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "execution.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Model Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, get the spesifc item we want to perfrom prediction with our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "item_filename = '/Black-grass/6.png'\n",
        "\n",
        "item = dataset.items.get(filepath=item_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the model predict method on the item.\n",
        "\n",
        "Notice: Since the predict method expect to recive a list of item ids, we will add the item id to a list that will be sent to the method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clone_model.predict(item_ids=[item.id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Model Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice: To evaluate a model we need to:\n",
        "1. Get a dataset, that will be used for the evaluation.\n",
        "2. Define the test filters, to build the test dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_filters = dl.Filters(field='metadata.system.tags.test', values=True)\n",
        "\n",
        "clone_model.evaluate(dataset_id=dataset.id,\n",
        "                     filters=test_filters)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
