{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "  \n",
                "## 4. Upload Files  \n",
                "  \n",
                "The following script upload images we’re prepared in a single folder located on the machine running this notebook. Alternatively you can:\n",
                "1. Run this or any other script locally to upload your own files from your machine\n",
                "2. Drag & drop files on the Dataset-browser page\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "import dtlpy as dl\n",
                "# project_id is automatically set by the notebook \n",
                "project_id = '2bb16c5f-081f-4afb-91e0-78699c1b3476'\n",
                "# dataset_id is automatically set by the notebook\n",
                "dataset_id = '66cc292f71079277b19a30c4'\n",
                "local_path = './images/other/'\n",
                "remote_path = '/first_images'\n",
                "\n",
                "# add comment that this is autopopulated\n",
                "project = dl.projects.get(project_id=project_id)\n",
                "dataset = project.datasets.get(dataset_id=dataset_id)\n",
                "items = dataset.items.upload(local_path=local_path,\n",
                "                     remote_path=remote_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Reminder:** When connecting your cloud storage, files are not uploaded, they are only indexed while the actual binary files remain in your storage.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false
            },
            "source": [
                "  \n",
                "  \n",
                "## 5. Refresh the page\n",
                "  \n",
                "Click the refresh button to see the files uploaded to the dataset\n",
                "\n",
                "<style>\n",
                "    button {\n",
                "        background-color: #3452ff;\n",
                "        color: #ffffff;\n",
                "        border: none;\n",
                "        border-radius: 4px;\n",
                "        padding: 10px 20px;\n",
                "        cursor: pointer;\n",
                "        font-size: 14px;\n",
                "        font-weight: 500;\n",
                "        transition: background-color 0.3s;\n",
                "    }\n",
                "\n",
                "    button:hover {\n",
                "        background-color: #7b8cff;\n",
                "    }\n",
                "</style>\n",
                "\n",
                "<button id=\"RefreshButtonJupyter\">Refresh the page</button>\n",
                " \n",
                "  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "  \n",
                "## 6. Add Metadata\n",
                "\n",
                "Up-tp 100 different searchable meta-date keys can be added to files in a single datasets. Those meta-data keys can then be used to search and filter data while curating the dataset, or even while streaming data through Pipelines. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "for item in items:\n",
                "    item.metadata['user'] = {'source':'jupyter_notebook'}\n",
                "    item.update()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Upload Annotations \n",
                "Once again, this example upload annotations we’ve prepared alongside the images in the machine running the notebook. \n",
                "\n",
                "Alternatively you can run this or any other script locally, to load annotation from your local machine. You can then experience with other annotation types, such frame-based annotation onto video files, text annotations onto documents, prompt/response and more.\n",
                " \n",
                "  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "\n",
                "# First item:\n",
                "local_path1 = r'./images/snake/8dc203e3a9.jpg'\n",
                "local_annotations_path1 = r\"./images/snake/8dc203e3a9.json\"\n",
                "remote_path1 = '/'\n",
                "remote_name1 = 'snake.jpg'\n",
                "item_metadata1 = {'user': {'type': 'snake_image'}}\n",
                "# Second item:\n",
                "local_path2 = r\"./images/bird/0f67ffd92d.jpg\"\n",
                "local_annotations_path2 = r\"./images/bird/0f67ffd92d.jpg\"\n",
                "remote_path2 = \"/\"\n",
                "remote_name2 = 'bird.jpg'\n",
                "item_metadata2 = {'user': {'type': 'bird_image'}}\n",
                "\n",
                "to_upload = list()\n",
                "# First item and info attached:\n",
                "to_upload.append({'local_path': local_path1,  # Local path to image\n",
                "                  'local_annotations_path': local_annotations_path1,  # Local path to annotation file\n",
                "                  'remote_path': remote_path1,  # Remote directory of uploaded image\n",
                "                  'remote_name': remote_name1,  # Remote name of image\n",
                "                  'item_metadata': item_metadata1})  # Metadata for the created item\n",
                "# Second item and info attached:\n",
                "to_upload.append({'local_path': local_path2,  # Local path to image\n",
                "                  'local_annotations_path': local_annotations_path2,  # Local path to annotation file\n",
                "                  'remote_path': remote_path2,  # Remote directory of uploaded image\n",
                "                  'remote_name': remote_name2,  # Remote name of image\n",
                "                  'item_metadata': item_metadata2})  # Metadata for the created item\n",
                "df = pd.DataFrame(to_upload)  # Make data into DF table\n",
                "items = dataset.items.upload(local_path=df,\n",
                "                             overwrite=True)  # Upload DF to platform\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Arrange Data in Folders\n",
                "\n",
                "Create folders and move file items into them, or upload file items directly into a folder. Up-to 1,000 folders in a single dataset enabled arranging data by your context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "directory = '/snakes'\n",
                "\n",
                "dataset.items.make_dir(directory=directory)\n",
                "\n",
                "item = dataset.items.get(filepath='/snake.jpg')\n",
                "item.move(new_path=directory)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Filter Data\n",
                "\n",
                "Dataloop Query Language allows you to search and filter data based on any item and annotation attributes, including meta-data and feature-vectors. \n",
                "The following example uses the files and meta-data we prepared in advance to search for all files with annotations and add them to a ‘Train set’."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filters = dl.Filters()\n",
                "filters.add(field='annotated', values=True)\n",
                "pages = dataset.items.list(filters=filters)\n",
                "\n",
                "items = [item for page in pages for item in page]\n",
                "\n",
                "for item in items:\n",
                "    print(item.filename)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Add Feature Vectors\n",
                "\n",
                "You can install an embedder model from the Dataloop marketplace to extract the features from all the files in the dataset. Alternatively, if you extract features using proprietary models, you can use the SDK to upload features directly onto an item. Here, we will upload dummy vector."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "feature_set_name = 'onboarding_feature_set'\n",
                "try:\n",
                "   feature_set = project.feature_sets.get(feature_set_name=feature_set_name)\n",
                "   print('Feature set already exists')\n",
                "except dl.exceptions.NotFound:\n",
                "   print('Creating feature set')\n",
                "   feature_set = project.feature_sets.create(name=feature_set_name,\n",
                "                                                    entity_type=dl.FeatureEntityType.ITEM,\n",
                "                                                    project_id=project.id,\n",
                "                                                    set_type='onboarding',\n",
                "                                                    size=10)\n",
                "   \n",
                "dummy_embeddings = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
                "\n",
                "item = items[0]\n",
                "feature = feature_set.features.create(value=dummy_embeddings, entity=item)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Export Data\n",
                "If you prefer training your models in environments other than Dataloop you can easily export everything following Dataloop JSON format, or use a converter to export in one of the common formats like COCO. You can export for all files in the Dataset, or again, use a filter to export for selected files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filters = dl.Filters()\n",
                "filters.add(field='metadata.user.source', values='jupyter_notebook')\n",
                "dataset.items.download(filters=filters)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Reminder:** The command above will download everything to the cloud machine running the Jupyter notebook, not to your local machine."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
